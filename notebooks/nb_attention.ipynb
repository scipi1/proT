{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "ROOT_DIR = \"../\"\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.einsum\n",
    "[documentation](https://pytorch.org/docs/stable/generated/torch.einsum.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "seq_len = 2          # sequence length (here we assume the same for input/output, but it can be different)\n",
    "heads = 3            # number of heads (multi-head attention)\n",
    "d_model = 12         # Embedding dimension (?)\n",
    "d_query_key = 9      # Dimension for the query and key vectors\n",
    "d_values = 10        # Dimension for the value vector\n",
    "\n",
    "x = torch.ones(BATCH_SIZE,seq_len,d_model) # d_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights are the same\n"
     ]
    }
   ],
   "source": [
    "# single-head VS multi-head\n",
    "x = torch.ones(BATCH_SIZE,seq_len,d_model)\n",
    "one_projection = nn.Linear(d_model, d_query_key, bias=False)\n",
    "multi_projection = nn.Linear(d_model, d_query_key * heads, bias=False)\n",
    "\n",
    "# set the same weights\n",
    "same_weights = multi_projection.weight[:d_query_key,:]\n",
    "one_projection.weight = nn.Parameter(same_weights)\n",
    "\n",
    "if (one_projection.weight-same_weights).all() == 0:\n",
    "    print(\"The weights are the same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection to Query-Key dimension\n",
    "The input is projected from its embedding dimension `embed_dim` to the query-key dimension `d_queries_keys`. In case of multi-head attention, the latter is multiplied by the number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1, 2, 12]), single head shape: torch.Size([1, 2, 9]), multi head shape: torch.Size([1, 2, 27])\n"
     ]
    }
   ],
   "source": [
    "one_x = one_projection(x)\n",
    "multi_x = multi_projection(x)\n",
    "print(f\"x shape: {x.shape}, single head shape: {one_x.shape}, multi head shape: {multi_x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0128,  0.2430, -0.7559,  0.4436, -0.3671,  1.3778, -0.5534,\n",
       "           1.1814,  0.5273],\n",
       "         [ 0.0128,  0.2430, -0.7559,  0.4436, -0.3671,  1.3778, -0.5534,\n",
       "           1.1814,  0.5273]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0128,  0.2430, -0.7559,  0.4436, -0.3671,  1.3778, -0.5534,\n",
       "           1.1814,  0.5273, -0.2040,  0.7508, -0.1226,  0.3260, -0.9922,\n",
       "          -0.1010, -0.4803, -0.0498, -1.0036,  0.5103, -0.1023,  0.3187,\n",
       "          -0.2637,  0.0280,  0.0977,  0.3184,  0.2986, -0.9544],\n",
       "         [ 0.0128,  0.2430, -0.7559,  0.4436, -0.3671,  1.3778, -0.5534,\n",
       "           1.1814,  0.5273, -0.2040,  0.7508, -0.1226,  0.3260, -0.9922,\n",
       "          -0.1010, -0.4803, -0.0498, -1.0036,  0.5103, -0.1023,  0.3187,\n",
       "          -0.2637,  0.0280,  0.0977,  0.3184,  0.2986, -0.9544]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the first `d_queries_keys` are the same for both tensors. The successive heads are concatenated along the last dimension.\n",
    "\n",
    "*Q: How do I know that different heads learn different patterns?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "$$Attention = Softmax\\Bigg(\\frac{Q\\cdot K^T}{\\sqrt{d}}\\Bigg)V$$\n",
    "### 1. Query and Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 27])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input shape (BATCH_SIZE,seq_len,d_model) \n",
    "# not sure it's correct...where are the features? It's all in the embedding?\n",
    "\n",
    "x = torch.rand(BATCH_SIZE,seq_len,d_model) # d_model = embed_dim (?)\n",
    "\n",
    "query_projection = nn.Linear(d_model, d_query_key * heads)\n",
    "key_projection = nn.Linear(d_model, d_query_key * heads)\n",
    "\n",
    "B, L, _ = x.shape\n",
    "query = query_projection(x)\n",
    "key = key_projection(x)\n",
    "\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Separate heads\n",
    "So far the heads are all concatenated along the last dimension. Using [view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html), we can separate separate the heads, each one of them with its `d_query_key` components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: BATCH_SIZE (1), seq_len (2), d_query_key*heads (27) --> torch.Size([1, 2, 27])\n",
      "New shape: BATCH_SIZE (1), seq_len (2), heads (3), d_query_key (27) --> torch.Size([1, 2, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "query_view = query.view((B, L, heads, -1))\n",
    "key_view = key.view((B, L, heads, -1))\n",
    "\n",
    "print(f\"Old shape: BATCH_SIZE ({BATCH_SIZE}), seq_len ({seq_len}), d_query_key*heads ({d_query_key*heads}) --> {query.shape}\")\n",
    "print(f\"New shape: BATCH_SIZE ({BATCH_SIZE}), seq_len ({seq_len}), heads ({heads}), d_query_key ({d_query_key*heads}) --> {query_view.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scaling\n",
    "The \"scaled\" dot-product attention is scaled by $\\sqrt{d_{query, key}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1.0 / sqrt(d_query_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Attention scores\n",
    "This is the center of the dot-product attention, where the similarity between query and keys are calculated. We obtain the **attention matrix**, which gives insights on the degree of correlation between query and key (*cross-attention* input/output, *self-attention* input/input).\n",
    "\n",
    "*Q: does it works also for different input-output sizes? Yes because the dot product is taken along the key-queries dimension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score shapes: BATCH_SIZE (1), heads (3), seq_len (2), seq_len (2) --> torch.Size([1, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.einsum(\"blhe,bshe->bhls\", query_view, key_view)\n",
    "print(f\"Score shapes: BATCH_SIZE ({BATCH_SIZE}), heads ({heads}), seq_len ({seq_len}), seq_len ({seq_len}) --> {scores.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Scale and apply Softmax\n",
    "Softmax converts velues into probability distribution, since it normalize each columns into values $\\in [0,1]$. It is apply to the last dimension of our tensor, since it correspond to the columns of the attention sub-matrix.\n",
    "\n",
    "> This step is the reason why previous attempts to reduce the square-complexity of attention fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This step doesn't change the shape: torch.Size([1, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "A = torch.softmax(scale * scores, dim=-1)\n",
    "print(f\"This step doesn't change the shape: {A.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Multiply by the values\n",
    "\n",
    "The multiplication by values represent the feedback for the embedding, based on what was calculated by the attention. It should be considered as a unique linear map, from the current embedding to the new suggestions from the attention.\n",
    "\n",
    "`d_model` $\\rightarrow$ `d_model`\n",
    "\n",
    "But, in practice, it's implemented in two steps\n",
    "\n",
    "`d_model`$\\rightarrow$`d_values`$\\times$`heads` $\\rightarrow$ `d_values`$\\times$`heads`$\\rightarrow$ `d_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 30])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just for comparison \n",
    "# query_projection.shape = (d_model, d_query_key * heads)\n",
    "\n",
    "value_projection = nn.Linear(d_model, d_values *heads)\n",
    "out_projection = nn.Linear(d_values*heads, d_model)\n",
    "\n",
    "values = value_projection(x)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Separate heads\n",
    "As done before for query and key..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N.B. In case query and keys have different seq_lengths, \n",
    "# it has to be set = to keys (columns)\n",
    "\n",
    "values_view = values.view(BATCH_SIZE, seq_len, heads, -1)\n",
    "values_view.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Multiply with einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.einsum(\"bhls,bshd->blhd\", A, values_view)\n",
    "V = V.contiguous()\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Concatenate the heads back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 30])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_view = V.view(BATCH_SIZE,seq_len,-1)\n",
    "V_view.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Multiply by the Output\n",
    "This last multiplication, maps the output back to the initial shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 12])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out_projection(V_view)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Attention\n",
    "$$A(x_q)=\\sum_{x_k} \\frac{k(x_q,x_k)}{Z}v(x_k)$$\n",
    "With the normalization constant $Z = \\sum_{x_k} k(x_q,x_k')$\n",
    "\n",
    "\n",
    "- The kernel $k$ from *Tsai (i) Positional Embeddings*\n",
    "$$k(x_q,x_k)=k_\\text{exp}(f_q+t_q,f_k+t_k)$$ \n",
    "\n",
    "where $k_\\text{exp}$ is the exponential kernel \n",
    "\n",
    "$k_\\text{exp}(q,k)=\\exp{\\bigg(\\frac{\\langle qW_q,kW_k\\rangle}{\\sqrt{d_k}}\\bigg)}$\n",
    "\n",
    "- The value function $v(x_k)=(f_k+t_k)W_v$ is pretty much tha same as the dot-product attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.8513e-04, 4.1025e-01, 4.0401e-07, 5.8956e-01],\n",
       "         [3.0236e-08, 9.6120e-07, 9.9875e-01, 1.2447e-03]],\n",
       "\n",
       "        [[4.4123e-10, 2.2233e-04, 2.1314e-11, 9.9978e-01],\n",
       "         [9.9636e-01, 1.7948e-05, 7.2879e-05, 3.5478e-03]],\n",
       "\n",
       "        [[3.0797e-01, 9.8793e-09, 1.2706e-04, 6.9190e-01],\n",
       "         [4.5116e-02, 1.2714e-10, 1.0660e-09, 9.5488e-01]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def k_exp(Q,K,d_k):\n",
    "    return torch.exp(torch.einsum(\"ble,bke->blk\", Q, K)/sqrt(d_k))\n",
    "    \n",
    "q_size = 2\n",
    "k_size = 4\n",
    "v_size = q_size\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "x_q = torch.randn(BATCH_SIZE,q_size, d_model)\n",
    "x_k = torch.randn(BATCH_SIZE,k_size, d_model)\n",
    "\n",
    "W_k = torch.randn(BATCH_SIZE,d_model, d_query_key)\n",
    "W_q = torch.randn(BATCH_SIZE,d_model, d_query_key)\n",
    "W_v = torch.randn(BATCH_SIZE,d_model, d_model)\n",
    "\n",
    "\n",
    "K = x_k@W_k\n",
    "Q = x_q@W_q\n",
    "V = x_k@W_v  \n",
    "\n",
    "ker = k_exp(Q,K,d_query_key)\n",
    "\n",
    "# normalization constant\n",
    "Z = ker.sum(axis=2).unsqueeze(-1).expand(-1,-1,k_size)\n",
    "\n",
    "k_cross_att = (ker/Z)\n",
    "att_out = k_cross_att@V\n",
    "k_cross_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "\n",
      "x_keys: torch.Size([3, 4, 12])\n",
      "x_queries: torch.Size([3, 2, 12])\n",
      "Keys: torch.Size([3, 4, 9])\n",
      "Queries: torch.Size([3, 2, 9])\n",
      "kernel: torch.Size([3, 2, 4])\n",
      "Cross_attention_coefficients: torch.Size([3, 2, 4])\n",
      "Attention output: torch.Size([3, 2, 12])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes:\\n\")\n",
    "print(f\"x_keys: {x_k.shape}\")\n",
    "print(f\"x_queries: {x_q.shape}\")\n",
    "print(f\"Keys: {K.shape}\")\n",
    "print(f\"Queries: {Q.shape}\")\n",
    "print(f\"kernel: {ker.shape}\")\n",
    "print(f\"Cross_attention_coefficients: {k_cross_att.shape}\")\n",
    "print(f\"Attention output: {att_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000],\n",
       "        [1.0000, 1.0000],\n",
       "        [1.0000, 1.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization\n",
    "k_cross_att.sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_cross_att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True, False, False],\n",
       "         [False,  True, False, False]],\n",
       "\n",
       "        [[False,  True, False, False],\n",
       "         [False,  True, False, False]],\n",
       "\n",
       "        [[False,  True, False, False],\n",
       "         [False,  True, False, False]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.rand(k_size)>0.5\n",
    "mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "mask=mask.expand_as(k_cross_att)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One to One Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    nan,  0.1039, -0.3482],\n",
       "         [-1.4673,  0.8101, -0.7187]],\n",
       "\n",
       "        [[-0.6928, -0.4300,  0.3263],\n",
       "         [ 0.4130, -0.9167,  1.4718]],\n",
       "\n",
       "        [[ 0.4810, -0.0921,  0.2670],\n",
       "         [ 0.5295, -1.0155,  1.2435]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_emb1 = 3\n",
    "d_emb2 = 5\n",
    "\n",
    "d_emb_list = [d_emb1,d_emb2]\n",
    "\n",
    "X_emb1 = torch.randn(BATCH_SIZE,seq_len,d_emb1)\n",
    "X_emb1[0,0,0] = float('nan')\n",
    "X_emb2 = torch.zeros(BATCH_SIZE,seq_len,d_emb2).fill_(float('nan'))\n",
    "X_emb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    nan,  0.1039, -0.3482],\n",
       "         [-1.4673,  0.8101, -0.7187]],\n",
       "\n",
       "        [[-0.6928, -0.4300,  0.3263],\n",
       "         [ 0.4130, -0.9167,  1.4718]],\n",
       "\n",
       "        [[ 0.4810, -0.0921,  0.2670],\n",
       "         [ 0.5295, -1.0155,  1.2435]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_emb = torch.cat((X_emb1,X_emb2), dim=-1)\n",
    "X_emb_split = torch.split(X_emb,d_emb_list,dim=-1)\n",
    "X_emb_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=3, out_features=9, bias=True)\n",
       "  (1): Linear(in_features=5, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_projections = nn.ModuleList([nn.Linear(d_emb, d_query_key)for d_emb in d_emb_list])\n",
    "query_projections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "               nan,     nan],\n",
       "          [-0.4774, -0.7712,  0.7957, -0.1341, -0.2469, -0.9042, -0.4872,\n",
       "           -0.7373,  0.3509]],\n",
       " \n",
       "         [[ 0.2119, -0.1436,  0.6397, -0.0292,  0.3975, -0.4922,  0.1334,\n",
       "           -0.8205, -0.0744],\n",
       "          [ 0.6324,  0.1488,  0.1885,  0.6374,  0.9332,  0.2457,  0.3951,\n",
       "           -0.3664, -0.1188]],\n",
       " \n",
       "         [[ 0.7270, -0.1419,  0.1202,  0.6506,  0.1419,  0.1347, -0.2498,\n",
       "           -0.0755,  0.3711],\n",
       "          [ 0.8513,  0.2342,  0.1881,  0.5598,  0.8071,  0.2293,  0.3608,\n",
       "           -0.3808, -0.1034]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       " \n",
       "         [[nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       " \n",
       "         [[nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
       "        grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [proj(emb) for proj, emb in zip(query_projections, X_emb_split)]\n",
    "keys = [proj(emb) for proj, emb in zip(query_projections, X_emb_split)]\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000],\n",
       "         [0.0000, 3.2566]],\n",
       "\n",
       "        [[1.5726, 0.8269],\n",
       "         [0.8269, 2.0997]],\n",
       "\n",
       "        [[1.2305, 1.0181],\n",
       "         [1.0181, 2.1182]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [torch.nan_to_num(torch.matmul(q, k.transpose(-2, -1))) for q,k in zip(queries,keys)]\n",
    "sum(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "isnan(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39misnan(scores)\n\u001b[0;32m      2\u001b[0m scores\u001b[38;5;241m.\u001b[39mmasked_fill_(mask,\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m      3\u001b[0m scores\n",
      "\u001b[1;31mTypeError\u001b[0m: isnan(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "mask = torch.isnan(scores)\n",
    "scores.masked_fill_(mask,-torch.inf)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.8513e-04,       -inf, 4.0401e-07, 5.8956e-01],\n",
       "         [3.0236e-08,       -inf, 9.9875e-01, 1.2447e-03]],\n",
       "\n",
       "        [[4.4123e-10,       -inf, 2.1314e-11, 9.9978e-01],\n",
       "         [9.9636e-01,       -inf, 7.2879e-05, 3.5478e-03]],\n",
       "\n",
       "        [[3.0797e-01,       -inf, 1.2706e-04, 6.9190e-01],\n",
       "         [4.5116e-02,       -inf, 1.0660e-09, 9.5488e-01]]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class UniformAttentionMask(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(UniformAttentionMask,self).__init__()\n",
    "    \n",
    "    def forward(self, attention_scores:torch.Tensor, mask:torch.Tensor):\n",
    "        \"\"\"\n",
    "        Applies masking to the attention scores.\n",
    "        \n",
    "        Args:\n",
    "        - attention_scores: Tensor of shape (batch_size, N_queries, N_keys).\n",
    "        - mask: Boolean tensor of shape (N_keys), where False means the corresponding key should be masked (zeroed).\n",
    "        \n",
    "        Returns:\n",
    "        - masked_attention_scores: Tensor with masked attention scores.\n",
    "        \"\"\"\n",
    "\n",
    "        assert attention_scores.shape[-1] == len(mask), AssertionError(f\"Got mask of length {len(mask)}, expected {attention_scores.shape[-1]}\")\n",
    "        \n",
    "        # Ensure the mask is a torch tensor\n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.tensor(mask)\n",
    "        \n",
    "        # Ensure the mask is on the same device as the attention scores\n",
    "        if mask.device != attention_scores.device:\n",
    "            mask = mask.to(attention_scores.device)\n",
    "        \n",
    "        # Convert boolean mask to float and expand it to match attention_scores\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, N_keys)\n",
    "        mask=mask.expand_as(attention_scores)\n",
    "        # Apply the mask to zero out the attention scores where mask is False\n",
    "        \n",
    "        return attention_scores.masked_fill(mask, -torch.inf)\n",
    "    \n",
    "uniform_mask = UniformAttentionMask()\n",
    "\n",
    "mask = np.random.rand(k_size)>0.5\n",
    "\n",
    "uniform_mask(k_cross_att,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'b': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\"a\":0}\n",
    "b = {\"b\":1}\n",
    "\n",
    "a | b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prochain_transformer.modules.extra_layers import UniformAttentionMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_layer = UniformAttentionMask()\n",
    "scores = torch.einsum(\"blhe,bshe->bhls\", query_view, key_view)\n",
    "A = torch.nan_to_num(torch.softmax(scores, dim=-1))\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prochain_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
