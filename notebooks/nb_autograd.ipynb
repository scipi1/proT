{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "d_model = 12\n",
    "trg_len = 1\n",
    "BATCH_SIZE = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0870,  1.4679, -0.3964,  0.3793,  0.5043,  0.9177,  0.7692,\n",
       "          -0.1930, -0.7334,  0.8734, -0.3822,  0.1088]],\n",
       "\n",
       "        [[-0.9930,  0.6887, -1.5292, -0.3639, -0.1337, -0.9362, -1.3257,\n",
       "          -0.0454,  0.3081, -0.7320,  0.1541, -0.5236]],\n",
       "\n",
       "        [[-0.9973,  0.3035, -1.2115, -1.8725, -1.1464,  0.3051,  0.7094,\n",
       "           0.4515,  0.5623, -0.6957, -1.0634,  0.4820]],\n",
       "\n",
       "        [[-0.4527, -1.5459, -0.2140,  0.7529, -0.7922, -0.2014, -0.3007,\n",
       "          -0.7866,  0.7521,  0.3239, -0.0395,  0.5355]],\n",
       "\n",
       "        [[ 0.0168,  0.5925, -1.3263,  1.0311, -0.7999, -0.1513,  0.3619,\n",
       "          -0.5641, -0.1595, -0.5207,  0.1579, -0.3582]]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.randn(BATCH_SIZE, trg_len, d_model, requires_grad=True)  # For simplicity, a 3-dimensional input\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(V):\n",
    "    return torch.sin(V)  # Example non-linear function\n",
    "\n",
    "Y = g(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 12])\n",
      "torch.Size([60])\n"
     ]
    }
   ],
   "source": [
    "print(V.shape)\n",
    "print(V.reshape(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source link](https://gist.github.com/apaszke/226abdf867c4e9d6698bd198f3b45fb7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 2., 0., 0.],\n",
      "        [0., 0., 4., 0.],\n",
      "        [0., 0., 0., 6.]])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 2., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 4., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 6.]]])\n"
     ]
    }
   ],
   "source": [
    "def jacobian(y, x, create_graph=False):                                                               \n",
    "    jac = []                                                                                          \n",
    "    flat_y = y.reshape(-1)                                                                            \n",
    "    grad_y = torch.zeros_like(flat_y)                                                                 \n",
    "    for i in range(len(flat_y)):                                                                      \n",
    "        grad_y[i] = 1.                                                                                \n",
    "        grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph)\n",
    "        jac.append(grad_x.reshape(x.shape))                                                           \n",
    "        grad_y[i] = 0.                                                                                \n",
    "    return torch.stack(jac).reshape(y.shape + x.shape)                                                \n",
    "                                                                                                      \n",
    "def hessian(y, x):                                                                                    \n",
    "    return jacobian(jacobian(y, x, create_graph=True), x)                                             \n",
    "                                                                                                      \n",
    "def f(x):                                                                                             \n",
    "    return x * x * torch.arange(4, dtype=torch.float)                                                 \n",
    "                                                                                                      \n",
    "x = torch.ones(4, requires_grad=True)\n",
    "                                                                 \n",
    "print(jacobian(f(x), x))                                                                              \n",
    "print(hessian(f(x), x))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Compute gradient of each y component with respect to V\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     grad_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(y, V, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m     jacobian\u001b[38;5;241m.\u001b[39mappend(grad_y)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Stack the results to form the Jacobian matrix\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\prochain_transformer\\Lib\\site-packages\\torch\\autograd\\__init__.py:384\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    377\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    381\u001b[0m     )\n\u001b[0;32m    383\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[1;32m--> 384\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _make_grads(\n\u001b[0;32m    385\u001b[0m     t_outputs, grad_outputs_, is_grads_batched\u001b[38;5;241m=\u001b[39mis_grads_batched\n\u001b[0;32m    386\u001b[0m )\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\prochain_transformer\\Lib\\site-packages\\torch\\autograd\\__init__.py:132\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         )\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m    136\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    137\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# Initialize a list to collect gradients for each component of Y\n",
    "jacobian = []\n",
    "\n",
    "for y in Y:\n",
    "    # Compute gradient of each y component with respect to V\n",
    "    grad_y = torch.autograd.grad(y, V, retain_graph=True, create_graph=True)[0]\n",
    "    jacobian.append(grad_y)\n",
    "\n",
    "# Stack the results to form the Jacobian matrix\n",
    "jacobian = torch.stack(jacobian)  # Shape: [output_dim, input_dim]\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prochain_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
