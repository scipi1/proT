{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "\n",
    "#load just the first batch\n",
    "X = torch.tensor(np.load(\"../data/input/dyconex_252901/X.npy\",allow_pickle=True).astype(float),dtype=torch.float)[:bs]\n",
    "Y = torch.tensor(np.load(\"../data/input/dyconex_252901/Y.npy\",allow_pickle=True).astype(float))[:bs]\n",
    "\n",
    "var_vocab_filename = \"../data/input/dyconex_252901/variables_vocabulary.json\"\n",
    "pos_vocab_filename = \"../data/input/dyconex_252901/position_vocabulary.json\"\n",
    "\n",
    "with open(var_vocab_filename) as f:\n",
    "    var_vocab = json.load(f)\n",
    "with open(pos_vocab_filename) as f:\n",
    "    pos_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_idx = 0\n",
    "process_idx = 1\n",
    "var_idx = 2\n",
    "pos_idx = 3\n",
    "val_idx = 4\n",
    "time_idx = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "$$\\mathcal{\\Phi}=\\text{concat}(v,\\phi_\\text{v},\\phi_\\text{t},\\phi_\\text{p})$$\n",
    "\n",
    "- idea: add also embedding for value with powers? e.g $av,bv^2,cv^3,dv^4$ where $a,b,c,d$ are learnable parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1453, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         ...,\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         ...,\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         ...,\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         ...,\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         ...,\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         ...,\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.logical_not(X[:,:,val_idx].isnan()).unsqueeze(-1)\n",
    "print(mask.shape)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1453, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_mask = mask.expand(-1,1453,10)\n",
    "print(exp_mask.shape)\n",
    "exp_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable-Value Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3358, 1453, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_var_emb = 3\n",
    "var_vocab_size = len(var_vocab)\n",
    "var_emb = nn.Embedding(var_vocab_size,d_var_emb)\n",
    "var_embed_idx = torch.nan_to_num(X[:,:,var_idx]).type(torch.int)\n",
    "var_emb(var_embed_idx).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Identity()\n",
    "len(X[:,:,val_idx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3358, 1453, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3035,  0.2188,  0.3355,  0.4864],\n",
       "         [ 0.6504, -1.5821, -1.4712,  0.5450],\n",
       "         [ 0.6370, -0.2556, -0.6311,  2.2377],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864]],\n",
       "\n",
       "        [[ 0.3035,  0.2188,  0.3355,  0.4864],\n",
       "         [ 0.6504, -1.5821, -1.4712,  0.5450],\n",
       "         [ 0.6370, -0.2556, -0.6311,  2.2377],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864]],\n",
       "\n",
       "        [[ 0.3035,  0.2188,  0.3355,  0.4864],\n",
       "         [ 0.6504, -1.5821, -1.4712,  0.5450],\n",
       "         [ 0.6370, -0.2556, -0.6311,  2.2377],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan, -1.5821, -1.4712,  0.5450],\n",
       "         [    nan, -0.2556, -0.6311,  2.2377],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864]],\n",
       "\n",
       "        [[    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan, -1.5821, -1.4712,  0.5450],\n",
       "         [    nan, -0.2556, -0.6311,  2.2377],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864]],\n",
       "\n",
       "        [[    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan, -1.5821, -1.4712,  0.5450],\n",
       "         [    nan, -0.2556, -0.6311,  2.2377],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864],\n",
       "         [    nan,  0.2188,  0.3355,  0.4864]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_val_emb = torch.cat((X[:,:,val_idx].unsqueeze(-1),var_emb(var_embed_idx)),dim=-1)\n",
    "print(var_val_emb.shape)\n",
    "var_val_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input - PaPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pos_emb = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target - sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4385, -0.3589, -0.6868, -1.0939,  0.8427],\n",
       "         [-0.7602,  1.0481, -0.5870, -0.1887,  1.2394],\n",
       "         [ 0.3057,  0.6494, -0.8286,  0.0674,  0.1681],\n",
       "         ...,\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315]],\n",
       "\n",
       "        [[ 1.4385, -0.3589, -0.6868, -1.0939,  0.8427],\n",
       "         [-0.7602,  1.0481, -0.5870, -0.1887,  1.2394],\n",
       "         [ 0.3057,  0.6494, -0.8286,  0.0674,  0.1681],\n",
       "         ...,\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315]],\n",
       "\n",
       "        [[ 1.4385, -0.3589, -0.6868, -1.0939,  0.8427],\n",
       "         [-0.7602,  1.0481, -0.5870, -0.1887,  1.2394],\n",
       "         [ 0.3057,  0.6494, -0.8286,  0.0674,  0.1681],\n",
       "         ...,\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.4385, -0.3589, -0.6868, -1.0939,  0.8427],\n",
       "         [-0.7602,  1.0481, -0.5870, -0.1887,  1.2394],\n",
       "         [ 0.3057,  0.6494, -0.8286,  0.0674,  0.1681],\n",
       "         ...,\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315]],\n",
       "\n",
       "        [[ 1.4385, -0.3589, -0.6868, -1.0939,  0.8427],\n",
       "         [-0.7602,  1.0481, -0.5870, -0.1887,  1.2394],\n",
       "         [ 0.3057,  0.6494, -0.8286,  0.0674,  0.1681],\n",
       "         ...,\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315]],\n",
       "\n",
       "        [[ 1.4385, -0.3589, -0.6868, -1.0939,  0.8427],\n",
       "         [-0.7602,  1.0481, -0.5870, -0.1887,  1.2394],\n",
       "         [ 0.3057,  0.6494, -0.8286,  0.0674,  0.1681],\n",
       "         ...,\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315],\n",
       "         [-0.6453,  1.5275,  1.9571,  0.9368, -0.3315]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pos_trg = int(Y[:,:,pos_idx].flatten().nan_to_num().max()+1)\n",
    "pos_trg_emb = nn.Embedding(max_pos_trg,d_pos_emb)\n",
    "pos_embed_idx = torch.nan_to_num(Y[:,:,pos_idx]).type(torch.int)\n",
    "phi_pos_trg = pos_trg_emb(pos_embed_idx)\n",
    "phi_pos_trg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Embeddings\n",
    "#### Time2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, input_dim:int=6, embed_dim:int=512, activation=torch.sin):\n",
    "        super(Time2Vec, self).__init__()\n",
    "        \n",
    "        assert embed_dim % input_dim == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim // input_dim # so that the final dimension is embed_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        # initialize learnable weights and biases\n",
    "        self.embed_weight = nn.parameter.Parameter(torch.rand(self.input_dim,self.embed_dim))\n",
    "        self.embed_bias = nn.parameter.Parameter(torch.rand(self.input_dim,self.embed_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "            \n",
    "        x_diag = torch.diag_embed(x)\n",
    "        #print(f\"Xdiag: {x_diag.shape} emb: {self.embed_weight.shape}\")\n",
    "        # x.shape = (bs, sequence_length, input_dim, input_dim)\n",
    "        x_affine = torch.matmul(x_diag, self.embed_weight) + self.embed_bias\n",
    "        # x_affine.shape = (bs, sequence_length, input_dim, time_embed_dim)\n",
    "        x_affine_0, x_affine_remain = torch.split(x_affine, [1, self.embed_dim - 1], dim=-1)\n",
    "        x_affine_remain = self.activation(x_affine_remain)\n",
    "        x_out = torch.cat([x_affine_0, x_affine_remain], dim=-1)\n",
    "        x_out = x_out.view(x_out.size(0), x_out.size(1), -1)\n",
    "        return x_out\n",
    "\n",
    "d_time = 5\n",
    "time_emb_dim = 5\n",
    "time2vec_emb = Time2Vec(d_time, embed_dim=time_emb_dim*d_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3358, 1453, 25])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_emb = time2vec_emb(X[:,:,time_idx:])\n",
    "time_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3035,  0.2188,  0.3355,  ..., -0.7933,  0.4403, -0.7082],\n",
       "         [ 0.6504, -1.5821, -1.4712,  ..., -0.7933,  0.4403, -0.7082],\n",
       "         [ 0.6370, -0.2556, -0.6311,  ..., -0.7933,  0.4403, -0.7082],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        [[ 0.3035,  0.2188,  0.3355,  ..., -0.7933,  0.4403, -0.7082],\n",
       "         [ 0.6504, -1.5821, -1.4712,  ..., -0.7933,  0.4403, -0.7082],\n",
       "         [ 0.6370, -0.2556, -0.6311,  ..., -0.7933,  0.4403, -0.7082],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        [[ 0.3035,  0.2188,  0.3355,  ..., -0.7933,  0.4403, -0.7082],\n",
       "         [ 0.6504, -1.5821, -1.4712,  ..., -0.7933,  0.4403, -0.7082],\n",
       "         [ 0.6370, -0.2556, -0.6311,  ..., -0.7933,  0.4403, -0.7082],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan, -1.5821, -1.4712,  ...,     nan,     nan,     nan],\n",
       "         [    nan, -0.2556, -0.6311,  ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        [[    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan, -1.5821, -1.4712,  ...,     nan,     nan,     nan],\n",
       "         [    nan, -0.2556, -0.6311,  ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        [[    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan, -1.5821, -1.4712,  ...,     nan,     nan,     nan],\n",
       "         [    nan, -0.2556, -0.6311,  ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan],\n",
       "         [    nan,  0.2188,  0.3355,  ...,     nan,     nan,     nan]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((var_val_emb,time_emb),dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6169, -0.2145,  1.9997],\n",
       "         [    nan,  1.2196,  0.7723],\n",
       "         [ 0.8318, -0.2092, -0.2938],\n",
       "         [ 0.9720, -0.4684, -0.2061],\n",
       "         [ 0.0520,  1.2877, -0.0369]],\n",
       "\n",
       "        [[-0.7380,  0.1939, -0.9375],\n",
       "         [-1.2621,  0.7015, -0.1555],\n",
       "         [ 0.5386,  0.1562,  0.5686],\n",
       "         [-1.9712, -1.3248, -1.1392],\n",
       "         [    nan, -1.2711, -0.0214]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2\n",
    "L = 5\n",
    "d_emb = 3\n",
    "K = torch.randn(n,L,d_emb)\n",
    "Q = torch.randn(n,L,d_emb)\n",
    "\n",
    "\n",
    "# NaNs are found along L\n",
    "K[0,1,0]=torch.nan\n",
    "K[1,4,0]=torch.nan\n",
    "Q[0,0,0]=torch.nan\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    nan, -1.2260,  0.3419],\n",
       "         [ 0.9186, -0.2038, -0.7597],\n",
       "         [ 0.5567, -0.8945,  0.2760],\n",
       "         [-1.7344, -0.2097, -1.0253],\n",
       "         [-2.1004, -1.6605,  0.6282]],\n",
       "\n",
       "        [[ 1.7459, -1.7729,  0.7039],\n",
       "         [ 0.8620,  2.2945,  0.3033],\n",
       "         [-1.4279,  0.2278, -0.2120],\n",
       "         [-0.1705,  1.3434, -1.2678],\n",
       "         [-1.1626,  0.8139,  0.3990]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, 0., 0., 0.],\n",
       "         [0., -inf, 0., 0., 0.],\n",
       "         [0., -inf, 0., 0., 0.],\n",
       "         [0., -inf, 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., -inf]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get NaNs index (note: we don't need the hidden dimension, just n and L)\n",
    "K_nan_idx = torch.isnan(K).nonzero()[:,:-1]\n",
    "Q_nan_idx = torch.isnan(Q).nonzero()[:,:-1]\n",
    "\n",
    "# build M nxLXd\n",
    "M = torch.zeros(n,L,L) \n",
    "M[K_nan_idx.squeeze()[0],:,K_nan_idx.squeeze()[1]] = -torch.inf # cols (keys)\n",
    "M[Q_nan_idx.squeeze()[0],Q_nan_idx.squeeze()[1],:] = -torch.inf # rows (queries)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   -inf, -1.4133, -4.2192, -3.9464,  1.7540],\n",
       "         [   -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "         [   -inf, -0.8350,  1.0886,  1.1942,  0.0087],\n",
       "         [   -inf, -1.2412,  1.1841,  1.1058,  0.4848],\n",
       "         [   -inf,  0.6910, -1.0564,  0.4646, -2.2671]],\n",
       "\n",
       "        [[-2.4890,  2.2201,  0.6146, -1.4403, -2.1004],\n",
       "         [-2.1997,  1.4332, -0.5478, -1.3633, -3.0777],\n",
       "         [ 1.7515, -1.8988, -0.2463,  0.8939,  1.4359],\n",
       "         [-5.3736,  6.8307, -0.3649, -2.3456, -4.7514],\n",
       "         [   -inf,    -inf,    -inf,    -inf,    -inf]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K = torch.nan_to_num(K)\n",
    "# Q = torch.nan_to_num(Q)\n",
    "A = torch.matmul(K,Q.view(n,d_emb,L))\n",
    "A.masked_fill_(torch.isnan(A),-torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 4.0189e-02, 2.4294e-03, 3.1912e-03, 9.5419e-01],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 5.6248e-02, 3.8505e-01, 4.2793e-01, 1.3077e-01],\n",
       "         [0.0000e+00, 3.5241e-02, 3.9840e-01, 3.6838e-01, 1.9797e-01],\n",
       "         [0.0000e+00, 4.9418e-01, 8.6100e-02, 3.9406e-01, 2.5658e-02]],\n",
       "\n",
       "        [[7.2170e-03, 8.0075e-01, 1.6079e-01, 2.0598e-02, 1.0644e-02],\n",
       "         [2.1385e-02, 8.0880e-01, 1.1157e-01, 4.9358e-02, 8.8884e-03],\n",
       "         [4.3192e-01, 1.1224e-02, 5.8584e-02, 1.8322e-01, 3.1505e-01],\n",
       "         [5.0046e-06, 9.9913e-01, 7.4922e-04, 1.0337e-04, 9.3235e-06],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using nan_to_num() after the softmax solves two problems\n",
    "# 1) remove missing queries\n",
    "# 2) forces to zero the output when the whole input is masked (i.e. missing)\n",
    "score = F.softmax(A,dim=-1).nan_to_num()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    nan, -1.5955,  0.5663],\n",
       "         [    nan,  0.0000,  0.0000],\n",
       "         [    nan, -0.6628, -0.2930],\n",
       "         [    nan, -0.7695, -0.1701],\n",
       "         [    nan, -0.3030, -0.7395]],\n",
       "\n",
       "        [[ 0.4574,  1.8975,  0.1920],\n",
       "         [ 0.5565,  1.9168,  0.1777],\n",
       "         [ 0.2826, -0.2241,  0.1884],\n",
       "         [ 0.8602,  2.2928,  0.3028],\n",
       "         [ 0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(score,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prochain_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
