{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S6 Mamba backend: mamba-pytorch (pure PyTorch fallback)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT\\proT\\baseline\\s6\\s6_wrapper.py:13: UserWarning: ⚠️  Using pure PyTorch implementation of Mamba (slower but functional). For faster performance on GPU clusters, install mamba-ssm with CUDA support: pip install mamba-ssm causal-conv1d\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, abspath,join\n",
    "from os import makedirs\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "root_path = dirname(abspath(\"./\"))\n",
    "import sys\n",
    "sys.path.append(root_path)\n",
    "from proT.evaluation.predict import *\n",
    "\n",
    "# plotting standard settings\n",
    "plt.rcParams['figure.dpi'] = 100 #360 # standard is 360 but for set to 100 for practical visualization on the notebook\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "plt.rcParams['figure.figsize'] = (6, 6)\n",
    "plt.rcParams['lines.linewidth'] = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: documentation\n",
    "import torch\n",
    "from torchmetrics.regression import R2Score, MeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_df_torchmetrics(\n",
    "    y_pred: torch.Tensor,          # (B, S, D)\n",
    "    y_true: torch.Tensor,          # (B, S, D)\n",
    "    mask: torch.Tensor | None = None,  # (B, S) bool, True = valid\n",
    "    feature_names: list[str] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    if not isinstance(y_pred, torch.Tensor):\n",
    "        print(\"Converted output to torch tensor\")\n",
    "        y_pred = torch.tensor(y_pred)\n",
    "        \n",
    "    if not isinstance(y_true, torch.Tensor):\n",
    "        print(\"Converted target to torch tensor\")\n",
    "        y_true = torch.tensor(y_true)\n",
    "    \n",
    "    if y_pred.shape != y_true.shape or y_true.ndim != 3:\n",
    "        print(f\"Found output shape {y_pred.shape} and target shape {y_true.shape}\")\n",
    "        raise ValueError(\"Expected matching (B,S,D) tensors.\")\n",
    "    B, S, D = y_true.shape\n",
    "    device = y_true.device\n",
    "\n",
    "    if feature_names is not None and len(feature_names) != D:\n",
    "        raise ValueError(f\"feature_names length {len(feature_names)} must equal D={D}\")\n",
    "    \n",
    "    if D > 1:\n",
    "        labels = feature_names or [f\"f{d}\" for d in range(D)]\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    # Per-sample metrics (over S), returned per-feature (D)\n",
    "    r2_metric  = lambda: R2Score(multioutput=\"raw_values\").to(device)\n",
    "    mse_metric = lambda: MeanSquaredError().to(device)\n",
    "    mae_metric = lambda: MeanAbsoluteError().to(device)\n",
    "\n",
    "    rows = []\n",
    "    has_mask = mask is not None\n",
    "    if has_mask:\n",
    "        if mask.shape != (B, S):\n",
    "            raise ValueError(f\"mask must be (B,S); got {mask.shape}\")\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = mask[b] if has_mask else slice(None)\n",
    "        yt = y_true[b, idx, :]   # (S_b, D)\n",
    "        yp = y_pred[b, idx, :]   # (S_b, D)\n",
    "\n",
    "        if yt.numel() == 0:  # no valid steps → fill zeros\n",
    "            r2 = torch.zeros(D, device=device)\n",
    "            mse = torch.zeros(D, device=device)\n",
    "            mae = torch.zeros(D, device=device)\n",
    "        else:\n",
    "            r2  = r2_metric()(yp, yt)   # (D,)\n",
    "            mse = mse_metric()(yp, yt)  # (D,)\n",
    "            mae = mae_metric()(yp, yt)  # (D,)\n",
    "\n",
    "        r2  = r2.detach().cpu().tolist()\n",
    "        mse = mse.detach().cpu().tolist()\n",
    "        mae = mae.detach().cpu().tolist()\n",
    "\n",
    "        if D > 1 and labels is not None:\n",
    "            for d, name in enumerate(labels):\n",
    "                rows.append({\"index\": b, \"feature\": name, \"R2\": r2[d], \"MSE\": mse[d], \"MAE\": mae[d]})\n",
    "        else:\n",
    "            rows.append({\"index\": b, \"R2\": r2, \"MSE\": mse, \"MAE\": mae})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_predictions(\n",
    "    sample_id: int, var_index: int, x_index: int, val_index: int, target_array: np.ndarray, \n",
    "    output_array: np.ndarray, title_map: dict=None, \n",
    "    save_dir: str=None, tag: str=None):\n",
    "    \n",
    "    vars = np.unique(target_array[:,:,var_index])\n",
    "    vars = vars[~np.isnan(vars)]\n",
    "    num_vars = len(vars)\n",
    "    \n",
    "    fig = plt.figure(figsize=(6*num_vars, 6))\n",
    "    gs = gridspec.GridSpec(1, num_vars, wspace=0.3)\n",
    "\n",
    "    if output_array.shape[-1] > 1:\n",
    "        value = output_array[:,:,0].copy()\n",
    "    else:\n",
    "        value = output_array.copy()\n",
    "    \n",
    "    target = target_array.copy()\n",
    "    \n",
    "    # de-normalize\n",
    "    value *= 10\n",
    "    target[:, :, val_index] *= 10\n",
    "    \n",
    "    # get min and max value for xaxis_lim\n",
    "    c = np.concatenate([value[sample_id],target[sample_id, :, val_index]])\n",
    "    d = c[~(np.isnan(c))]\n",
    "    min_val = np.min(d)\n",
    "    max_val = np.max(d)\n",
    "    \n",
    "    for i,var in enumerate(vars):\n",
    "        var_mask = target[sample_id, :, var_index] == var\n",
    "        x = target[sample_id, :, x_index][var_mask]\n",
    "        y_out = value[sample_id][var_mask]\n",
    "        y_trg = target[sample_id, :, val_index][var_mask]\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        ax.plot(x, y_out, label=\"prediction\")\n",
    "        ax.plot(x, y_trg, label=\"target\")\n",
    "        ax.set_ylabel(r\"$\\Delta R [\\%]$\")\n",
    "        ax.set_xlabel(\"Cycle Number\")\n",
    "        ax.set_xlim(0,x[-1])\n",
    "        ax.set_ylim(min_val, max_val)\n",
    "        \n",
    "        # add metrics\n",
    "        mask = torch.logical_not(torch.tensor(target[sample_id,:,val_index][var_mask]).isnan())\n",
    "        df_met = eval_df_torchmetrics(\n",
    "            y_pred=torch.tensor(y_out).unsqueeze(0).unsqueeze(-1), \n",
    "            y_true=torch.tensor(y_trg).unsqueeze(0).unsqueeze(-1), mask=mask.unsqueeze(0))\n",
    "        mae = df_met.loc[0,\"MAE\"]\n",
    "        r2 = df_met.loc[0,\"R2\"]\n",
    "        \n",
    "        title = (title_map[var]) if title_map is not None else var\n",
    "        \n",
    "        ax.set_title(f\"{title}: MAE={mae:.2f}, R2={r2:.2f}\")\n",
    "        ax.legend()\n",
    "    \n",
    "    # optional export\n",
    "    filename = f\"prediction_sample_{sample_id}_{tag}.pdf\"\n",
    "    if save_dir is not None:\n",
    "        makedirs(save_dir, exist_ok=True)\n",
    "        out_path = join(save_dir, filename)\n",
    "        fig.savefig(out_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_attention(sample_id: int, cross_att_array: np.ndarray, input_array: np.ndarray, save_dir: str=None, tag: str=None):\n",
    "    \n",
    "    if len(cross_att_array.shape) == 4:\n",
    "        h_idx = 1\n",
    "        print(f\"MHA detected!\\nAttention shape: {cross_att_array.shape} summing on axis {h_idx}\")\n",
    "        attention = cross_att_array.sum(axis=1)\n",
    "        print(f\"new attention shape {attention.shape}\")\n",
    "    else:\n",
    "        attention = cross_att_array\n",
    "    \n",
    "    att = attention[sample_id]\n",
    "    N, M = att.shape\n",
    "    input_miss_bool = np.isnan(input_array[sample_id,:,val_index].squeeze())\n",
    "    input_miss = input_miss_bool[np.newaxis,:].astype(int)\n",
    "\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[10, 1], hspace=0.2)\n",
    "\n",
    "    # Main heatmap axis\n",
    "    ax0 = fig.add_subplot(gs[0])\n",
    "    divider0 = make_axes_locatable(ax0)\n",
    "    cax0 = divider0.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    im0 = ax0.imshow(att, cmap='viridis', aspect='auto', origin='lower')\n",
    "    \n",
    "    cbar = fig.colorbar(im0, cax=cax0, label='Attention Score Value')\n",
    "    cbar.formatter.set_powerlimits((0, 0))\n",
    "    cbar.update_ticks()\n",
    "\n",
    "    # Optional: Render the scientific notation with LaTeX for a cleaner look    \n",
    "    cbar.formatter.set_useMathText(True)\n",
    "    cbar.update_ticks()\n",
    "    \n",
    "    \n",
    "    ax0.set_xticks([])\n",
    "    ax0.set_ylabel(\"Queries\")\n",
    "    #ax0.set_title(\"Heatmap with Boolean Mask\")\n",
    "    \n",
    "    h, w = att.shape[:2]  # height, width\n",
    "\n",
    "    ax0.set_xticks([0, w-1])\n",
    "    ax0.set_xticklabels([0, w-1])\n",
    "    ax0.set_yticks([0, h-1])\n",
    "    ax0.set_yticklabels([0, h-1])\n",
    "    \n",
    "\n",
    "    # Boolean mask axis\n",
    "    ax1 = fig.add_subplot(gs[1], sharex=ax0)\n",
    "    divider1 = make_axes_locatable(ax1)\n",
    "    cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    im1 = ax1.imshow(input_miss, cmap='Greys', aspect='auto', origin='upper', vmin=0, vmax=1)\n",
    "    fig.colorbar(im1, cax=cax1, ticks=[0, 1], label='NAIM')\n",
    "    cax1.set_yticklabels(['0', r'$-\\infty$'])\n",
    "\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlabel(\"Keys\")\n",
    "    \n",
    "    # export\n",
    "    filename = f\"scores_sample_{sample_id}_{tag}.pdf\"\n",
    "    if save_dir is not None:\n",
    "        makedirs(save_dir, exist_ok=True)\n",
    "        out_path = join(save_dir, filename)\n",
    "        fig.savefig(out_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def get_group_score(sample_id: int, group_index: int, val_index: int, cross_att_array: np.ndarray, input_array: np.ndarray):\n",
    "    \n",
    "    cross_att = cross_att_array[sample_id]\n",
    "    cumul_scores = np.mean(cross_att, axis=0)\n",
    "    \n",
    "    group_array = np.unique(input_array[sample_id,:,group_index])\n",
    "    group_array = group_array[~np.isnan(group_array)]\n",
    "    count_dict = {}\n",
    "    for group in group_array:\n",
    "\n",
    "        mask = np.logical_and(\n",
    "            input_array[sample_id, :, group_index] == group, \n",
    "            ~np.isnan(input_array[sample_id, :, val_index]))\n",
    "        count_dict[group] = np.mean(cumul_scores[mask])\n",
    "    \n",
    "    return pd.DataFrame.from_dict(count_dict, orient='index').reset_index()\n",
    "\n",
    "\n",
    "\n",
    "def make_plots(\n",
    "    sample_id: int, var_index: int, target_array: np.ndarray, \n",
    "    output_array: np.ndarray, input_array: np.ndarray, cross_att_array: np.ndarray, title_map: dict=None, \n",
    "    save: bool=False):\n",
    "    \n",
    "    plot_predictions(sample_id, var_index, target_array, output_array, title_map, save)\n",
    "    plot_attention(sample_id, cross_att_array, input_array, save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\prochain_transformer\\Lib\\site-packages\\pytorch_lightning\\utilities\\migration\\utils.py:51: PossibleUserWarning: The loaded checkpoint was produced with Lightning v2.5.5, which is newer than your current Lightning version: v2.0.3\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Causal mask found!\n",
      "Loading pre-split data.\n",
      "Train input shape:  (12789, 20, 5)\n",
      "Train target shape:  (12789, 1, 4)\n",
      "Test input shape:  (3211, 20, 5)\n",
      "Test target shape:  (3211, 1, 4)\n",
      "Loading pre-split data.\n",
      "Train input shape:  (12789, 20, 5)\n",
      "Train target shape:  (12789, 1, 4)\n",
      "Test input shape:  (3211, 20, 5)\n",
      "Test target shape:  (3211, 1, 4)\n",
      "Test dataset selected.\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:05<00:00, 12.46it/s]\n"
     ]
    }
   ],
   "source": [
    "datadir_path = r\"../data/input/\"\n",
    "config_path = r\"../experiments/training/tests/protocol/test_baseline_proT_ishigami_sum/config_proT_ishigami_v5_2.yaml\"\n",
    "checkpoint_path = r\"../experiments/training/tests/protocol/test_baseline_proT_ishigami_sum/k_0/checkpoints/epoch0-initial.ckpt\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "results = predict_test_from_ckpt(config, datadir_path, checkpoint_path, cluster=False)\n",
    "\n",
    "# unpack\n",
    "input_array = results.inputs\n",
    "output_array = results.outputs\n",
    "target_array = results.targets\n",
    "cross_att_array = results.attention_weights[\"cross\"] \n",
    "enc_self_att_array = results.attention_weights[\"encoder\"]\n",
    "dec_self_att_array = results.attention_weights[\"decoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3211, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mtensor(target_array[\u001b[38;5;241m0\u001b[39m,:,\u001b[38;5;241m4\u001b[39m])\u001b[38;5;241m.\u001b[39misnan()\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "torch.tensor(target_array[0,:,4]).isnan().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = 4\n",
    "var_idx = 2\n",
    "\n",
    "t = torch.tensor(target_array[:,:,[val_idx,var_idx]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "values = t[..., 0]            # (B, L)\n",
    "vars   = t[..., 1].long()     # (B, L), categorical index\n",
    "C = 3            # int\n",
    "\n",
    "B, L = values.shape\n",
    "out = torch.zeros(B, L, C, device=values.device, dtype=values.dtype)\n",
    "\n",
    "# scatter along the last dim using vars as the index\n",
    "out.scatter_(dim=2, index=vars.unsqueeze(-1), src=values.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_torchmetrics(y_pred=output_array[:,:,0:1], y_true=target_array[:,:,4:5], mask=torch.logical_not(torch.tensor(target_array[:,:,4]).isnan()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = config[\"data\"][\"dataset\"][3:]\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "builds_dir = \"../../proT_pipeline/data/builds\"\n",
    "var_vocab_filepath = join(builds_dir, dataset_name, \"output/variables_vocabulary.json_trg\")\n",
    "pos_vocab_filepath = join(builds_dir, dataset_name, \"output/position_vocabulary.json\")\n",
    "rating_filepath = join(builds_dir, dataset_name, \"control/steps_selected.xlsx\")\n",
    "\n",
    "with open(var_vocab_filepath, 'r', encoding='utf-8') as file:\n",
    "    inv_var_dict = json.load(file)\n",
    "    \n",
    "with open(pos_vocab_filepath, 'r', encoding='utf-8') as file:\n",
    "    inv_pos_dict = json.load(file)\n",
    "\n",
    "df_rating = pd.read_excel(rating_filepath)\n",
    "\n",
    "# maps\n",
    "var_map = {v: k[:3] for k, v in inv_var_dict.items()}\n",
    "pos_map = {v: int(float(k)) for k, v in inv_pos_dict.items()}\n",
    "rate_map = df_rating.set_index(\"Step\")[\"Bewertung \"].to_dict()\n",
    "process_map = df_rating.set_index(\"Step\")[\"Process\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data indices\n",
    "var_index = 2\n",
    "x_index = 3\n",
    "val_index = 4\n",
    "\n",
    "# get variables\n",
    "vars = np.unique(target_array[:,:,var_index])\n",
    "vars = vars[~np.isnan(vars)]\n",
    "num_vars = len(vars)\n",
    "\n",
    "title_map = {\n",
    "    1.0: \"Sense A\",\n",
    "    2.0: \"Sense B\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 1\n",
    "\n",
    "plot_predictions(sample_id, var_index, x_index=3, val_index=4, target_array=target_array, output_array=output_array, title_map=title_map, save_dir=\"./figures\", tag=\"example\")\n",
    "\n",
    "#make_plots(sample_id, var_index, target_array, output_array, cross_att_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(sample_id, cross_att_array, input_array, save_dir=\"./figures\", tag=\"example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_index = x_index\n",
    "df = get_group_score(sample_id, group_index,val_index, cross_att_array, input_array)  \n",
    "df[\"PaPos\"] = df[\"index\"].map(pos_map)\n",
    "df[\"rating\"] = df[\"PaPos\"].map(rate_map)\n",
    "df[\"process\"] = df[\"PaPos\"].map(process_map)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"rating\"],df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prochain_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
