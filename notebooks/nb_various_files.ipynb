{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from os.path import dirname, abspath, join\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "parent_path = dirname(dirname(abspath(\"./\")))\n",
    "sys.path.append(parent_path)\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(join(config_path,\"config.yaml\"), 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "config = load_config(\"../experiments/new_embedding_test/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.sin'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"model\"][\"ds_embed_enc\"][1][\"kwargs\"][\"activation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'40.0': 0, '60.0': 1, '70.0': 2, '80.0': 3, '90.0': 4, '100.0': 5, '180.0': 6, '200.0': 7, '230.0': 8, '240.0': 9, '260.0': 10, '280.0': 11, '290.0': 12, '300.0': 13, '340.0': 14, '390.0': 15, '410.0': 16, '440.0': 17, '450.0': 18, '470.0': 19, '490.0': 20, '500.0': 21, '510.0': 22, '660.0': 23, '680.0': 24}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"../data/input/dyconex_252901/position_vocabulary.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'las_11': 0, 'las_12': 1, 'las_13': 2, 'las_14': 3, 'las_15': 4, 'las_16': 5, 'las_17': 6, 'las_18': 7, 'las_21': 8, 'las_22': 9, 'las_23': 10, 'las_28': 11, 'las_29': 12, 'las_30': 13, 'las_31': 14, 'las_32': 15, 'las_33': 16, 'las_34': 17, 'las_35': 18, 'las_36': 19, 'las_37': 20, 'las_38': 21, 'las_39': 22, 'las_40': 23, 'las_44': 24, 'las_45': 25, 'las_50': 26, 'las_51': 27, 'las_56': 28, 'las_57': 29, 'las_58': 30, 'las_59': 31, 'las_60': 32, 'las_61': 33, 'las_62': 34, 'las_63': 35, 'gal_100': 36, 'gal_101': 37, 'gal_102': 38, 'gal_104': 39, 'gal_105': 40, 'gal_106': 41, 'gal_107': 42, 'gal_109': 43, 'gal_110': 44, 'gal_112': 45, 'gal_113': 46, 'gal_114': 47, 'gal_115': 48, 'gal_116': 49, 'gal_117': 50, 'gal_118': 51, 'gal_119': 52, 'gal_12': 53, 'gal_121': 54, 'gal_122': 55, 'gal_123': 56, 'gal_124': 57, 'gal_126': 58, 'gal_127': 59, 'gal_129': 60, 'gal_130': 61, 'gal_132': 62, 'gal_133': 63, 'gal_135': 64, 'gal_136': 65, 'gal_137': 66, 'gal_138': 67, 'gal_140': 68, 'gal_141': 69, 'gal_143': 70, 'gal_144': 71, 'gal_145': 72, 'gal_146': 73, 'gal_147': 74, 'gal_148': 75, 'gal_150': 76, 'gal_151': 77, 'gal_153': 78, 'gal_154': 79, 'gal_155': 80, 'gal_156': 81, 'gal_158': 82, 'gal_159': 83, 'gal_160': 84, 'gal_161': 85, 'gal_165': 86, 'gal_166': 87, 'gal_167': 88, 'gal_168': 89, 'gal_169': 90, 'gal_170': 91, 'gal_171': 92, 'gal_172': 93, 'gal_173': 94, 'gal_174': 95, 'gal_175': 96, 'gal_178': 97, 'gal_179': 98, 'gal_182': 99, 'gal_183': 100, 'gal_184': 101, 'gal_185': 102, 'gal_186': 103, 'gal_187': 104, 'gal_188': 105, 'gal_191': 106, 'gal_192': 107, 'gal_193': 108, 'gal_196': 109, 'gal_197': 110, 'gal_199': 111, 'gal_20': 112, 'gal_200': 113, 'gal_201': 114, 'gal_202': 115, 'gal_205': 116, 'gal_206': 117, 'gal_209': 118, 'gal_210': 119, 'gal_211': 120, 'gal_212': 121, 'gal_213': 122, 'gal_214': 123, 'gal_215': 124, 'gal_218': 125, 'gal_219': 126, 'gal_22': 127, 'gal_220': 128, 'gal_223': 129, 'gal_224': 130, 'gal_226': 131, 'gal_227': 132, 'gal_228': 133, 'gal_229': 134, 'gal_23': 135, 'gal_232': 136, 'gal_233': 137, 'gal_236': 138, 'gal_237': 139, 'gal_238': 140, 'gal_241': 141, 'gal_242': 142, 'gal_245': 143, 'gal_246': 144, 'gal_247': 145, 'gal_248': 146, 'gal_249': 147, 'gal_25': 148, 'gal_250': 149, 'gal_254': 150, 'gal_255': 151, 'gal_256': 152, 'gal_257': 153, 'gal_258': 154, 'gal_259': 155, 'gal_26': 156, 'gal_260': 157, 'gal_263': 158, 'gal_264': 159, 'gal_265': 160, 'gal_268': 161, 'gal_271': 162, 'gal_272': 163, 'gal_273': 164, 'gal_274': 165, 'gal_277': 166, 'gal_278': 167, 'gal_28': 168, 'gal_281': 169, 'gal_282': 170, 'gal_283': 171, 'gal_286': 172, 'gal_287': 173, 'gal_29': 174, 'gal_290': 175, 'gal_291': 176, 'gal_292': 177, 'gal_293': 178, 'gal_294': 179, 'gal_295': 180, 'gal_296': 181, 'gal_299': 182, 'gal_300': 183, 'gal_301': 184, 'gal_304': 185, 'gal_305': 186, 'gal_307': 187, 'gal_308': 188, 'gal_309': 189, 'gal_310': 190, 'gal_313': 191, 'gal_314': 192, 'gal_317': 193, 'gal_318': 194, 'gal_319': 195, 'gal_322': 196, 'gal_323': 197, 'gal_326': 198, 'gal_327': 199, 'gal_328': 200, 'gal_331': 201, 'gal_332': 202, 'gal_335': 203, 'gal_336': 204, 'gal_337': 205, 'gal_338': 206, 'gal_339': 207, 'gal_34': 208, 'gal_340': 209, 'gal_341': 210, 'gal_344': 211, 'gal_345': 212, 'gal_346': 213, 'gal_349': 214, 'gal_35': 215, 'gal_350': 216, 'gal_351': 217, 'gal_352': 218, 'gal_353': 219, 'gal_354': 220, 'gal_355': 221, 'gal_358': 222, 'gal_362': 223, 'gal_363': 224, 'gal_364': 225, 'gal_367': 226, 'gal_368': 227, 'gal_37': 228, 'gal_371': 229, 'gal_372': 230, 'gal_373': 231, 'gal_374': 232, 'gal_375': 233, 'gal_376': 234, 'gal_377': 235, 'gal_38': 236, 'gal_380': 237, 'gal_381': 238, 'gal_382': 239, 'gal_40': 240, 'gal_41': 241, 'gal_43': 242, 'gal_44': 243, 'gal_46': 244, 'gal_47': 245, 'gal_48': 246, 'gal_49': 247, 'gal_50': 248, 'gal_51': 249, 'gal_53': 250, 'gal_54': 251, 'gal_56': 252, 'gal_57': 253, 'gal_59': 254, 'gal_60': 255, 'gal_61': 256, 'gal_62': 257, 'gal_64': 258, 'gal_65': 259, 'gal_66': 260, 'gal_67': 261, 'gal_68': 262, 'gal_69': 263, 'gal_70': 264, 'gal_71': 265, 'gal_72': 266, 'gal_73': 267, 'gal_75': 268, 'gal_76': 269, 'gal_77': 270, 'gal_78': 271, 'gal_79': 272, 'gal_80': 273, 'gal_82': 274, 'gal_83': 275, 'gal_85': 276, 'gal_86': 277, 'gal_88': 278, 'gal_89': 279, 'gal_9': 280, 'gal_91': 281, 'gal_92': 282, 'gal_93': 283, 'gal_96': 284, 'gal_97': 285, 'gal_99': 286, 'mic_10': 287, 'mic_11': 288, 'mic_12': 289, 'mic_13': 290, 'mic_14': 291, 'mic_15': 292, 'mic_16': 293, 'mic_17': 294, 'mic_22': 295, 'mic_23': 296, 'mic_24': 297, 'mic_25': 298, 'mic_26': 299, 'mic_27': 300, 'mic_28': 301, 'mic_29': 302, 'mic_30': 303, 'mic_31': 304, 'mic_32': 305, 'mic_33': 306, 'mic_34': 307, 'mic_35': 308, 'mic_36': 309, 'mic_37': 310, 'mic_38': 311, 'mic_39': 312, 'mic_40': 313, 'mic_41': 314, 'mic_50': 315, 'mic_51': 316, 'mic_52': 317, 'mic_53': 318, 'mic_70': 319, 'mic_75': 320, 'mul_10': 321, 'mul_100': 322, 'mul_101': 323, 'mul_102': 324, 'mul_103': 325, 'mul_104': 326, 'mul_105': 327, 'mul_106': 328, 'mul_107': 329, 'mul_108': 330, 'mul_109': 331, 'mul_11': 332, 'mul_110': 333, 'mul_111': 334, 'mul_112': 335, 'mul_113': 336, 'mul_114': 337, 'mul_115': 338, 'mul_116': 339, 'mul_117': 340, 'mul_118': 341, 'mul_119': 342, 'mul_12': 343, 'mul_120': 344, 'mul_121': 345, 'mul_122': 346, 'mul_123': 347, 'mul_124': 348, 'mul_125': 349, 'mul_126': 350, 'mul_127': 351, 'mul_128': 352, 'mul_129': 353, 'mul_13': 354, 'mul_130': 355, 'mul_14': 356, 'mul_15': 357, 'mul_16': 358, 'mul_17': 359, 'mul_18': 360, 'mul_19': 361, 'mul_20': 362, 'mul_21': 363, 'mul_22': 364, 'mul_23': 365, 'mul_24': 366, 'mul_25': 367, 'mul_26': 368, 'mul_27': 369, 'mul_28': 370, 'mul_29': 371, 'mul_30': 372, 'mul_31': 373, 'mul_32': 374, 'mul_33': 375, 'mul_34': 376, 'mul_35': 377, 'mul_36': 378, 'mul_37': 379, 'mul_38': 380, 'mul_39': 381, 'mul_40': 382, 'mul_41': 383, 'mul_42': 384, 'mul_43': 385, 'mul_44': 386, 'mul_45': 387, 'mul_46': 388, 'mul_51': 389, 'mul_52': 390, 'mul_53': 391, 'mul_54': 392, 'mul_59': 393, 'mul_60': 394, 'mul_61': 395, 'mul_62': 396, 'mul_63': 397, 'mul_64': 398, 'mul_65': 399, 'mul_66': 400, 'mul_67': 401, 'mul_68': 402, 'mul_69': 403, 'mul_70': 404, 'mul_71': 405, 'mul_72': 406, 'mul_73': 407, 'mul_74': 408, 'mul_75': 409, 'mul_76': 410, 'mul_77': 411, 'mul_78': 412, 'mul_83': 413, 'mul_84': 414, 'mul_85': 415, 'mul_86': 416, 'mul_87': 417, 'mul_88': 418, 'mul_89': 419, 'mul_90': 420, 'mul_91': 421, 'mul_92': 422, 'mul_93': 423, 'mul_94': 424, 'mul_95': 425, 'mul_96': 426, 'mul_97': 427, 'mul_98': 428, 'mul_99': 429, 'pla_10': 430, 'pla_15': 431, 'pla_16': 432, 'pla_17': 433, 'pla_18': 434, 'pla_19': 435, 'pla_20': 436, 'pla_21': 437, 'pla_22': 438, 'pla_23': 439, 'pla_25': 440, 'pla_26': 441, 'pla_27': 442, 'pla_28': 443, 'pla_29': 444, 'pla_30': 445, 'pla_31': 446, 'pla_32': 447, 'pla_33': 448, 'pla_34': 449, 'pla_35': 450, 'pla_37': 451, 'pla_38': 452, 'pla_39': 453, 'pla_40': 454, 'pla_41': 455, 'pla_42': 456, 'pla_43': 457, 'pla_44': 458, 'pla_45': 459, 'pla_46': 460, 'pla_47': 461, 'pla_48': 462, 'pla_49': 463, 'pla_50': 464, 'pla_51': 465, 'pla_52': 466, 'pla_53': 467, 'pla_54': 468, 'pla_55': 469, 'pla_56': 470, 'pla_57': 471, 'pla_59': 472, 'pla_60': 473, 'pla_61': 474, 'pla_62': 475, 'pla_63': 476, 'pla_64': 477, 'pla_65': 478, 'pla_66': 479, 'pla_67': 480, 'pla_68': 481, 'pla_69': 482, 'pla_7': 483, 'pla_70': 484, 'pla_8': 485, 'pla_9': 486, 'delta_A_norm': 487, 'delta_B_norm': 488}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/input/dyconex_252901/variables_vocabulary.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2022.0, 2022.0, 2022.0, ..., nan, nan, nan],\n",
       "       [2022.0, 2022.0, 2022.0, ..., nan, nan, nan],\n",
       "       [2022.0, 2022.0, 2022.0, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load(\"../data/input/dyconex_252901/X.npy\", allow_pickle=True)\n",
    "X[:,:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.0220e+03, 1.0000e+00, 3.0000e+00, 1.6000e+01, 4.3000e+01],\n",
       "         [2.0220e+03, 1.0000e+00, 3.0000e+00, 1.6000e+01, 4.3000e+01],\n",
       "         [2.0220e+03, 1.0000e+00, 3.0000e+00, 1.6000e+01, 4.3000e+01],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan]],\n",
       "\n",
       "        [[2.0220e+03, 1.0000e+00, 3.0000e+00, 1.6000e+01, 4.3000e+01],\n",
       "         [2.0220e+03, 1.0000e+00, 3.0000e+00, 1.6000e+01, 4.3000e+01],\n",
       "         [2.0220e+03, 1.0000e+00, 3.0000e+00, 1.6000e+01, 4.3000e+01],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan]],\n",
       "\n",
       "        [[2.0220e+03, 1.0000e+00, 3.0000e+00, 1.6000e+01, 4.3000e+01],\n",
       "         [2.0220e+03, 1.0000e+00, 3.0000e+00, 1.6000e+01, 4.3000e+01],\n",
       "         [2.0220e+03, 1.0000e+00, 3.0000e+00, 1.6000e+01, 4.3000e+01],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan]],\n",
       "\n",
       "        [[       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan]],\n",
       "\n",
       "        [[       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan],\n",
       "         [       nan,        nan,        nan,        nan,        nan]]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor(X.astype(\"float16\"))\n",
    "X[:,:,[5,6,7,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.nan_to_num(X.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 2: 486.0\n",
      "Index 3: 24.0\n"
     ]
    }
   ],
   "source": [
    "def get_vocab_size(ix):\n",
    "    print(f\"Index {ix}: {X[:,:,ix].flatten().max()}\")\n",
    "\n",
    "get_vocab_size(2)\n",
    "get_vocab_size(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m emb(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\prochain_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\prochain_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\prochain_transformer\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\prochain_transformer\\Lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "emb(torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prochain_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
