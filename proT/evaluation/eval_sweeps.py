import numpy as np
import pandas as pd
from pathlib import Path
from omegaconf import OmegaConf
from os import makedirs, listdir
from os.path import join, isdir, abspath, dirname, exists
from typing import List, Callable, Union, Dict
import pandas as pd
import shutil
# import papermill as pm
from typing import List, Callable
import pandas as pd
import os
import warnings
from pathlib import Path
import boto3
from botocore import UNSIGNED
from botocore.config import Config
from io import StringIO, BytesIO
import torch
import re

os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"







# condition to identify the bottom level________________________________________________________________________________________________

def has_logs_subfolder(directory: str, s3: bool = False, bucket: str = None) -> bool:
    """
    Check if the given directory contains at least one logs subfolder
    """
    target_folder = "logs"

    if s3:
        s3_client = get_s3_client()
        paginator = s3_client.get_paginator("list_objects_v2")
        pages = paginator.paginate(Bucket=bucket, Prefix=directory, Delimiter='/')

        for page in pages:
            for prefix in page.get("CommonPrefixes", []):
                if prefix["Prefix"].rstrip('/').endswith(target_folder):
                    return True
        return False

    else:
        directory_path = Path(directory)
        return any(subdir.name == target_folder for subdir in directory_path.iterdir() if subdir.is_dir())


def has_kfold_summary(directory: str, s3: bool = False, bucket: str = None) -> bool:
    """
    Check if the given directory contains kfold_summary.json file.
    This indicates we've reached a trained model directory (bottom level).
    
    Args:
        directory: Directory path to check
        s3: Whether to check on S3
        bucket: S3 bucket name (required if s3=True)
        
    Returns:
        bool: True if kfold_summary.json exists in directory
    """
    target_file = "kfold_summary.json"

    if s3:
        s3_client = get_s3_client()
        # Ensure directory ends with /
        prefix = directory.rstrip('/') + '/'
        file_key = prefix + target_file
        
        try:
            s3_client.head_object(Bucket=bucket, Key=file_key)
            return True
        except:
            return False
    else:
        directory_path = Path(directory)
        return (directory_path / target_file).exists()


# bottom actions________________________________________________________________________________________________

def eval_models_bottom_action(filepath: str, level_folders: List[str], s3: bool = False, bucket: str = None, datadir_path: str = None) -> pd.DataFrame:
    """
    Bottom action that evaluates trained models by:
    - Loading config and kfold_summary.json from each folder
    - Finding the best checkpoint based on kfold results
    - Running predictions on test set
    - Computing metrics
    - Returning results as a tidy DataFrame
    
    This function is designed to be used as a bottom_action callable with get_df_recursive.
    It keeps metadata extraction minimal (only folder name) - level information 
    (level_0, level_1, ...) is automatically added by get_df_recursive.
    
    Args:
        filepath: Current directory path
        level_folders: List of subdirectories at this level
        s3: Whether files are on S3
        bucket: S3 bucket name (required if s3=True)
        datadir_path: Path to data directory. If None, uses "../data/input"
        
    Returns:
        pd.DataFrame: Metrics results with columns:
            - index: sample index
            - feature: feature name (if multivariate)
            - R2, MSE, MAE, RMSE: metric values
            - model_folder: folder name containing the model
            
    Example:
        >>> df = get_df_recursive(
        ...     filepath="experiments/ds_size",
        ...     bottom_action=eval_models_bottom_action,
        ...     is_bottom=has_kfold_summary
        ... )
        >>> # Result will have level_0, level_1, etc. columns added automatically
    """
    from proT.evaluation.predict import predict_test_from_ckpt
    from proT.evaluation.metrics import compute_prediction_metrics
    
    # Default data directory path
    if datadir_path is None:
        datadir_path = "../data/input"
    
    df_list = []
    
    for folder in level_folders:
        if s3:
            # S3 paths
            folder_path = f"{filepath.rstrip('/')}/{folder}"
            kfold_summary_key = f"{folder_path}/kfold_summary.json"
            config_key = f"{folder_path}/config.yaml"
            
            # Note: S3 implementation would need additional work to download files
            # and handle checkpoint loading from S3
            print(f"Warning: S3 support for eval_models_bottom_action not fully implemented")
            continue
            
        else:
            # Local paths
            folder_path = join(filepath, folder)
            kfold_summary_path = join(folder_path, "kfold_summary.json")
            config_path = join(folder_path, "config.yaml")
            
            # Validate required files exist
            if not exists(config_path):
                print(f"Warning: No config.yaml in {folder_path}, skipping...")
                continue
                
            if not exists(kfold_summary_path):
                print(f"Warning: No kfold_summary.json in {folder_path}, skipping...")
                continue
            
            try:
                # Load config and kfold summary
                config = OmegaConf.load(config_path)
                kfold_summary = OmegaConf.load(kfold_summary_path)
                best_fold_number = kfold_summary.best_fold.fold_number
                
                # Build checkpoint path
                checkpoint_path = join(
                    folder_path,
                    f'k_{best_fold_number}',
                    'checkpoints',
                    'best_checkpoint.ckpt'
                )
                
                if not exists(checkpoint_path):
                    print(f"Warning: Checkpoint not found: {checkpoint_path}, skipping...")
                    continue
                
                print(f"Processing {folder}...")
                print(f"  Config: {config_path}")
                print(f"  Checkpoint: {checkpoint_path}")
                
                # Run predictions
                results = predict_test_from_ckpt(
                    config=config,
                    datadir_path=datadir_path,
                    checkpoint_path=checkpoint_path,
                    dataset_label="test",
                    cluster=False
                )
                
                # Compute metrics
                # Try to get val_idx from config
                val_idx = None
                if "data" in config and "val_idx" in config["data"]:
                    val_idx = config["data"]["val_idx"]
                
                metrics_df = compute_prediction_metrics(
                    results,
                    target_feature_idx=val_idx
                )
                
                # Add folder name column for identification
                metrics_df["model_folder"] = folder
                
                df_list.append(metrics_df)
                print(f"  Successfully processed {folder}")
                
            except Exception as e:
                print(f"Error processing {folder}: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    # Concatenate all results
    if df_list:
        return pd.concat(df_list, ignore_index=True)
    else:
        print("Warning: No models were successfully processed")
        return pd.DataFrame()  # Empty dataframe if no models processed


# main recursive function________________________________________________________________________________________________

def get_df_recursive(filepath: str, bottom_action: Callable, is_bottom: Callable, s3: bool=False, bucket: str=None, lev: int=0)->pd.DataFrame:
    """
    Loops recursively inside folders, keeping track of the various levels, until the bottom is reached
    At the bottom, performs the bottom_action.
    
    N.B. The condition for the bottom is hard-coded

    Args:
        filepath (str): level path, if user input, starting level
        bottom_action (Callable): function to perform at the bottom level
        s3 (bool): AWS s3 flag
        lev (int, optional): Current level, leave default value. Defaults to 0.

    Returns:
        pd.DataFrame: multi-level dataframe
    """
    
    # files on s3 bucket
    if s3:
        
        s3_client = get_s3_client()
        
        # List all "directories" one level under the current prefix
        paginator = s3_client.get_paginator("list_objects_v2")
        pages = paginator.paginate(Bucket=bucket, Prefix=filepath, Delimiter='/')
        
        level_folders = []
        for page in pages:
            for prefix in page.get("CommonPrefixes", []):
                level_folders.append(prefix["Prefix"].rstrip('/').split('/')[-1])
        
        if not level_folders:
            return pd.DataFrame()  # empty folder
        
        # init dataframe
        df = None
        
        # Check each folder individually to handle mixed bottom/non-bottom scenarios
        for case in level_folders:
            subpath = f"{filepath.rstrip('/')}/{case}/"
            
            # Check if THIS specific folder is a bottom folder
            if is_bottom(subpath, s3=s3, bucket=bucket):
                print(f"reached bottom level: {case}")
                # Process this single bottom folder
                df_temp = bottom_action(filepath, [case], s3=s3, bucket=bucket)
                
                if df_temp is not None and not df_temp.empty:
                    df_temp[f"level_{lev}"] = case
                    df = df_temp if df is None else pd.concat([df, df_temp])
            else:
                # Not a bottom folder - try to recurse into it
                try:
                    df_temp = get_df_recursive(subpath, bottom_action, is_bottom, s3=s3, bucket=bucket, lev=lev+1)
                    
                    if df_temp is not None and not df_temp.empty:
                        df_temp[f"level_{lev}"] = case
                        df = df_temp if df is None else pd.concat([df, df_temp])
                except Exception as e:
                    print(f"Warning: Could not process {subpath}: {e}")
                    continue  # Skip this folder and continue with others
    
    # files on local machine
    else:
        
        # get all folders of current level
        level_folders = [d for d in listdir(filepath) if isdir(join(filepath,d))]
        
        if not level_folders:
            return pd.DataFrame()  # Empty directory
        
        # init dataframe
        df = None
        
        # Check each folder individually to handle mixed bottom/non-bottom scenarios
        for case in level_folders:
            filepath_ = join(filepath, case)
            
            # Check if THIS specific folder is a bottom folder
            if is_bottom(filepath_):
                print(f"reached bottom level: {case}")
                # Process this single bottom folder
                df_temp = bottom_action(filepath, [case], s3=s3, bucket=bucket)
                
                if df_temp is not None and not df_temp.empty:
                    df_temp[f"level_{lev}"] = case
                    df = df_temp if df is None else pd.concat([df, df_temp])
            else:
                # Not a bottom folder - try to recurse into it
                try:
                    df_temp = get_df_recursive(filepath_, bottom_action, is_bottom, s3=s3, bucket=bucket, lev=lev+1)
                    
                    if df_temp is not None and not df_temp.empty:
                        df_temp[f"level_{lev}"] = case
                        df = df_temp if df is None else pd.concat([df, df_temp])
                except Exception as e:
                    print(f"Warning: Could not process {filepath_}: {e}")
                    continue  # Skip this folder and continue with others

    return df if df is not None else pd.DataFrame()




def eval_sweeps(filepath: str, outpath: str = None, s3: bool = False, datadir_path: str = None):
    """
    Evaluates sweep experiment by recursively traversing directories, running model
    predictions and computing metrics at the bottom level.
    
    Args:
        filepath: Root directory containing experiment sweep
        outpath: Output directory to save results CSV. If None, only returns DataFrame without saving
        s3: Whether files are on S3 (default: False)
        datadir_path: Path to data directory (default: "../data/input")
        
    Returns:
        pd.DataFrame: Complete results with metrics and level information
        
    Example:
        >>> # Save results to file
        >>> df = eval_sweeps(
        ...     filepath="experiments/ds_size",
        ...     outpath="results",
        ...     datadir_path="../data/input"
        ... )
        >>> 
        >>> # Only return DataFrame without saving
        >>> df = eval_sweeps(
        ...     filepath="experiments/ds_size",
        ...     datadir_path="../data/input"
        ... )
    """
    # Create a wrapper for bottom_action that includes datadir_path
    def bottom_action_with_datadir(filepath, level_folders, s3=False, bucket=None):
        return eval_models_bottom_action(filepath, level_folders, s3, bucket, datadir_path)
    
    df = get_df_recursive(
        filepath=filepath, 
        bottom_action=bottom_action_with_datadir, 
        is_bottom=has_kfold_summary, 
        s3=s3, 
        bucket="scipi1-public"
    )
    
    # Save results if outpath is provided
    if outpath is not None:
        makedirs(outpath, exist_ok=True)
        output_path = join(outpath, "eval_sweeps.csv")
        df.to_csv(output_path, index=False)
        print(f"\nResults saved to {output_path}")
    
    print(f"Total rows: {len(df)}")
    print(f"Columns: {list(df.columns)}")
    
    return df



# helpers________________________________________________________________________________________________
def get_s3_client(public_only: bool = True):
    if public_only:
        return boto3.client("s3", config=Config(signature_version=UNSIGNED))
    else:
        return boto3.client("s3")
