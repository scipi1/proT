# proT Euler Sweep

Parameter sweep framework for proT models supporting independent and combination sweeps with sequential or parallel execution.

## Overview

This module provides systematic parameter exploration for proT experiments. It supports:
- Independent sweep: vary one parameter at a time
- Combination sweep: explore all combinations (Cartesian product)
- Sequential execution: local or cluster
- Parallel execution: SLURM job arrays on cluster

## Quick Start

### 1. Setup Experiment

Create experiment directory structure:

```bash
mkdir -p experiments/training/my_experiment/sweeper
```

Add configuration files:
- `experiments/training/my_experiment/config.yaml` - base configuration
- `experiments/training/my_experiment/sweeper/sweep.yaml` - sweep definition

### 2. Define Sweep Parameters

Example `sweeper/sweep.yaml`:

```yaml
# Independent sweep: 3 + 2 = 5 runs
# Combination sweep: 3 × 2 = 6 runs

model:
  kwargs:
    n_heads: [2, 4, 8]

training:
  lr: [0.0001, 0.001]
```

### 3. Run Sweep

Sequential (local or cluster):
```bash
python -m proT.euler_sweep.cli sweep \
    --exp_id my_experiment \
    --sweep_mode combination
```

Parallel (cluster only):
```bash
python -m proT.euler_sweep.cli sweep \
    --exp_id my_experiment \
    --sweep_mode combination \
    --parallel \
    --cluster \
    --scratch_path $SCRATCH/my_experiment
```

## Directory Structure

```
experiments/training/my_experiment/
├── config.yaml              # Base configuration
└── sweeper/
    ├── sweep.yaml          # Sweep definition
    └── runs/               # Generated by sweeper
        ├── combinations/   # Combination mode results
        │   ├── combo_n_heads_2_lr_0.0001/
        │   │   ├── config.yaml
        │   │   └── k_0/
        │   └── combo_n_heads_2_lr_0.001/
        │       └── ...
        └── sweeps/         # Independent mode results
            ├── sweep_n_heads/
            │   ├── sweep_n_heads_2/
            │   └── sweep_n_heads_4/
            └── sweep_lr/
                └── ...
```

## Sweep Modes

### Independent Sweep

Varies one parameter at a time, keeping others at default values.

Example:
- config: `n_heads=4, lr=0.001` (defaults)
- sweep: `n_heads=[2, 4, 8], lr=[0.0001, 0.001]`
- Creates 5 runs: baseline + 2 variations of n_heads + 2 variations of lr

### Combination Sweep

Tests all possible combinations (Cartesian product).

Example:
- sweep: `n_heads=[2, 4, 8], lr=[0.0001, 0.001]`
- Creates 6 runs: 3 × 2 combinations

Warning: Combinations grow exponentially. With N parameters having M values each: M^N runs.

## Execution Modes

### Sequential

Runs combinations one after another. Use for:
- Local execution
- Small sweeps (< 10 combinations)
- Cluster when parallel resources not needed

```bash
python -m proT.euler_sweep.cli sweep --exp_id exp --sweep_mode combination
```

### Parallel

Uses SLURM job arrays for parallel execution. Use for:
- Large sweeps (> 10 combinations)
- Cluster with SLURM scheduler

```bash
python -m proT.euler_sweep.cli sweep \
    --exp_id exp \
    --sweep_mode combination \
    --parallel \
    --cluster \
    --max_concurrent_jobs 10 \
    --walltime "3-00:00:00"
```

## CLI Options

Core options:
- `--exp_id`: Experiment folder name (required)
- `--sweep_mode`: `independent` or `combination` (required)
- `--parallel`: Enable parallel execution (default: False)
- `--cluster`: Running on cluster (default: False)
- `--scratch_path`: SCRATCH path for cluster (default: None)

SLURM options (parallel mode only):
- `--max_concurrent_jobs`: Max parallel jobs (default: 6)
- `--walltime`: Time limit (default: "5-00:00:00")
- `--gpu_mem`: GPU memory (default: "24g")
- `--mem_per_cpu`: CPU memory (default: "10g")

## Integration with proT

The sweep framework integrates with proT's training pipeline:

1. Loads base config from experiment directory
2. Applies sweep parameters
3. Calls `update_config()` for proT-specific preprocessing
4. Runs `trainer()` with k-fold cross-validation
5. Saves results in sweep directory structure

All proT features are supported:
- K-fold cross-validation
- All model types (proT, proT_sim, proT_adaptive, baselines)
- Config preprocessing with `update_config()`
- Cluster and local execution

## Comparison with Existing Implementation

### Old way (decorator-based):

```python
# In proT.cli
from proT.training.experiment_control import combination_sweep

@combination_sweep(exp_dir, mode="combination")
def run_sweep(config, save_dir):
    trainer(config=config, save_dir=save_dir, ...)
```

### New way (function-based):

```bash
python -m proT.euler_sweep.cli sweep --exp_id exp --sweep_mode combination
```

Advantages:
- Cleaner architecture (no decorators)
- Better modularity (separate module)
- Native parallel execution support
- Dry run capability
- Comprehensive logging

Both approaches can coexist. The old decorator-based method in `experiment_control.py` remains functional.

## Parallel Execution Details

When using `--parallel`:

1. Generates all parameter combinations
2. Saves metadata to `sweeper/combinations_data.json`
3. Creates SLURM script `sweeper/run_sweep_array.sh`
4. Submits job array to SLURM
5. Each job runs one combination using `sweep_worker.py`

Monitor progress:
```bash
squeue -u $USER              # Check job status
ls sweeper/runs/combinations/ | wc -l  # Count completed runs
tail sweeper/slurm_logs/sweep_*.out    # View logs
```

## Customization for Cluster

Edit generated `sweeper/run_sweep_array.sh`:
- Module loads (lines with `module load`)
- Virtual environment path (`VENV_PATH`)
- SLURM parameters (in `#SBATCH` directives)

## Example Workflows

### Small architectural sweep:
```bash
# 3 × 2 = 6 combinations
python -m proT.euler_sweep.cli sweep \
    --exp_id arch_sweep \
    --sweep_mode combination
```

### Large hyperparameter sweep:
```bash
# Many combinations, run in parallel
python -m proT.euler_sweep.cli sweep \
    --exp_id hyperparam_sweep \
    --sweep_mode combination \
    --parallel \
    --cluster \
    --max_concurrent_jobs 20
```

### Sensitivity analysis:
```bash
# Test each parameter independently
python -m proT.euler_sweep.cli sweep \
    --exp_id sensitivity \
    --sweep_mode independent
```

## Troubleshooting

**Error: Config file not found**
- Ensure `config.yaml` exists in experiment root directory

**Error: Sweeper directory not found**
- Create `sweeper/` subdirectory in experiment folder
- Add `sweep.yaml` inside sweeper directory

**Error: Parallel execution requires cluster**
- Add `--cluster` flag when using `--parallel`

**SLURM job fails**
- Check module loads in generated script
- Verify virtual environment path
- Review SLURM logs in `sweeper/slurm_logs/`

**Import errors in parallel mode**
- Ensure proT is installed in virtual environment
- Check Python path setup in SLURM script

## Related Modules

- `proT.euler_optuna`: Hyperparameter optimization with Optuna
- `proT.training.experiment_control`: Original sweep implementation (decorator-based)
- `proT.training.trainer`: Core training function with k-fold CV

## Architecture

Components:
- `sweeper.py`: Core sweep framework (combination generation, execution)
- `sweep_worker.py`: Worker script for parallel SLURM jobs
- `cli.py`: Command-line interface
- `__init__.py`: Module initialization

Pattern:
1. Combination generation → 2. Sweep execution → 3. Core training

This design maximizes code reuse and maintains clean separation of concerns.
